<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Model Use Cases</title>
    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <style>
        body { background-color: #1a1a1a; color: #e0e0e0; font-family: 'Arial', sans-serif; }
        .container { padding: 2rem 0; }
        h1 { color: #17a2b8; text-align: center; margin-bottom: 2rem; }
        h2 { color: #f0ad4e; margin-top: 2rem; margin-bottom: 1rem; }
        .model-card { background-color: #2a2a2a; border: 1px solid #3a3a3a; border-radius: 8px; padding: 1.5rem; margin-bottom: 1.5rem; transition: transform 0.2s, box-shadow 0.2s; }
        .model-card:hover { transform: translateY(-5px); box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3); }
        .model-card h3 { color: #17a2b8; margin-bottom: 0.5rem; }
        .model-card p { margin: 0.3rem 0; color: #b0b0b0; }
        @media (max-width: 576px) {
            .model-card { padding: 1rem; }
            h1 { font-size: 1.5rem; }
            h2 { font-size: 1.25rem; }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>AI Model Use Cases</h1>
        <!-- High-Performance Models -->
        <h2>High-Performance Models</h2>
        <div class="row">
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>vicuna:13b</h3>
                    <p><strong>Use Case:</strong> ShareGPT‑trained LLaMA‑1 variant; GPT‑3.5‑level chat quality in 13 B parameters—great for capable personal assistants.</p>
                </div>
            </div>
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>falcon2:11b</h3>
                    <p><strong>Use Case:</strong> 11 B multilingual model with 32 k context and 5 T‑token training—ideal for fast, large‑window content generation.</p>
                </div>
            </div>
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>falcon3:7b</h3>
                    <p><strong>Use Case:</strong> 7 B third‑gen Falcon offering 32 k context and strong zero‑shot reasoning for real‑time or edge deployments.</p>
                </div>
            </div>
        </div>
        <!-- Mistral Family -->
        <h2>Mistral Family</h2>
        <div class="row">
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>mistral-nemo:latest</h3>
                    <p><strong>Use Case:</strong> NVIDIA‑Mistral 12 B with 128 k context; single‑GPU reasoning, multilingual chat, and code generation powerhouse.</p>
                </div>
            </div>
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>mistral:latest</h3>
                    <p><strong>Use Case:</strong> Open 7 B model that beats Llama‑2‑13B on reasoning, math, and code while keeping memory demands low.</p>
                </div>
            </div>
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>mistral-small3.2:24b</h3>
                    <p><strong>Use Case:</strong> 24 B “Small 3.2” cut repetition and boosts tool‑use; fits on ~55 GB GPU for enterprise chatbots.</p>
                </div>
            </div>
        </div>
        <!-- LLaMA Family -->
        <h2>LLaMA Family</h2>
        <div class="row">
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>llama3.1:latest</h3>
                    <p><strong>Use Case:</strong> Llama 3.1 series with enhanced tokenizer and reasoning—flagship open‑weight foundation for research and production.</p>
                </div>
            </div>
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>llama3:8b</h3>
                    <p><strong>Use Case:</strong> Efficient 8 B variant rivaling larger Llama 2 models in QA, coding, and dialog on modest hardware.</p>
                </div>
            </div>
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>llama2-uncensored:7b</h3>
                    <p><strong>Use Case:</strong> Unfiltered Llama‑2‑7B fine‑tune for unrestricted creative generation and experimental prompts.</p>
                </div>
            </div>
        </div>
        <!-- DeepSeek Family -->
        <h2>DeepSeek Family</h2>
        <div class="row">
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>deepseek-coder:6.7b</h3>
                    <p><strong>Use Case:</strong> 6.7 B code LLM trained on 2 T tokens (87 % code); excels at completion, refactoring, and docs.</p>
                </div>
            </div>
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>DeepSeek-R1:14b</h3>
                    <p><strong>Use Case:</strong> 14 B RL‑trained reasoning model matching GPT‑3.5 performance for analytic chat and tool use.</p>
                </div>
            </div>
            <div class="col-12 col-md-4">
                <div class="model-card">
                    <h3>DeepSeek-R1:8b</h3>
                    <p><strong>Use Case:</strong> 8 B lightweight sibling of R1 delivering strong reasoning on edge devices and interactive apps.</p>
                </div>
            </div>
        </div>
        <!-- Gemma Family -->
        <h2>Gemma Family</h2>
        <div class="row">
            <div class="col-12 col-md-6">
                <div class="model-card">
                    <h3>gemma3:1b</h3>
                    <p><strong>Use Case:</strong> 1 B Gemma 3: on‑device summarization, Q&A, and multimodal tasks for mobile or IoT.</p>
                </div>
            </div>
            <div class="col-12 col-md-6">
                <div class="model-card">
                    <h3>gemma2:9b</h3>
                    <p><strong>Use Case:</strong> 9 B Gemma 2 balances capability and efficiency across general NLP and chat workloads.</p>
                </div>
            </div>
        </div>
        <!-- Dolphin Family -->
        <h2>Dolphin Family</h2>
        <div class="row">
            <div class="col-12 col-md-6">
                <div class="model-card">
                    <h3>dolphin-mistral:7b</h3>
                    <p><strong>Use Case:</strong> Dolphin 2.6 on Mistral 7B—uncensored chat and code assistant for local deployments.</p>
                </div>
            </div>
            <div class="col-12 col-md-6">
                <div class="model-card">
                    <h3>dolphin3:8b</h3>
                    <p><strong>Use Case:</strong> Dolphin 3 (Llama 3‑8B) aimed at general‑purpose local use with math, coding, and function calling.</p>
                </div>
            </div>
        </div>
        <!-- Specialized Models -->
        <h2>Specialized Models</h2>
        <div class="row">
            <div class="col-12 col-md-6">
                <div class="model-card">
                    <h3>wizardlm-uncensored:13b</h3>
                    <p><strong>Use Case:</strong> WizardLM‑13B without safety filters—great for brainstorming and educational “tutor” style interactions.</p>
                </div>
            </div>
            <div class="col-12 col-md-6">
                <div class="model-card">
                    <h3>medllama2:7b</h3>
                    <p><strong>Use Case:</strong> Medical‑domain Llama‑2 for clinical text understanding and research (non‑diagnostic).</p>
                </div>
            </div>
            <div class="col-12 col-md-6">
                <div class="model-card">
                    <h3>starcoder2:15b</h3>
                    <p><strong>Use Case:</strong> 15 B BigCode model trained on 600+ languages; top‑tier code completion and reasoning rivaling larger CodeLlama.</p>
                </div>
            </div>
        </div>
    </div>
    <!-- Bootstrap 5 JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
</body>
</html>
